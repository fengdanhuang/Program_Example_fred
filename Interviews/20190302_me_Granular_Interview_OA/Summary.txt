Summary.txt


---------------------------------------------------------------------
The code is based on python 3. 

First you need to put the data under your home directory.


The way to run the code:

$ python3 image_processing_pipeline.py

or using jupyter notebook to load image_processing_pipeline.ipynb and run it.

---------------------------------------------------------------------------------

In my Granular Image Processing Pipeline, the procedure is as follows:

1. Read images from subdirectories in dir 'imags_de'. Using open cv python functions.
   In each class, choose first 2/3 images set as training set; the rest 1/3 images set as test set.

2. Apply data augmentation feature provided in keras lib to do image augmentation

3. Build a convolutional neurual network model using keras lib 

4. After training, using evaluate function to test and and plot accuracy. 

5. Read exif info of each image using PIL lib. 

6. Put all latitude and logitude info into a dataframe.

7. Use google map api to show all geolocations.

----------------------------------------------------------------------------------

Specifically, the detail are as follows:

  * training data acquisition and storage
  Use  open cv functions to read each image and reshaping.
  Put all image data into numpy 4d array for training, 2d array for testing.
  
  * dataset curation
  For each class, true top 2/3 dataset as training, last 1/3 dataset as testing. training: 157 images. testing:77 images.

  * model development
  Use keras cnn to build model. Two conv layers and one connection layers. Details are shown in the code.

  * moving models to production
  We can use hadoop ecosystem to build the whole system. Build a oosie workflow to run the mode each day. Each day read last x days of data, then predict the current day's data.

  * serving predictions to the client
  Based on hadoop ecosystem, we can use database(elastic search, eg) to store all current day result. Then read the results to show it in UI/UX to the clients.

